{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe8e1c8-ab54-41f3-81e3-095d0b8f5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent mapping dictionary\n",
    "intent_map = {\n",
    "    \"look for\": \"zoom\",\n",
    "    \"find\": \"zoom\",\n",
    "    \"search for\": \"zoom\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510fba0a-2954-4a70-ab49-08d43e4aa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_intents(sentence, intent_map):\n",
    "    for phrase, keyword in intent_map.items():\n",
    "        if phrase in sentence:\n",
    "            sentence = sentence.replace(phrase, keyword)\n",
    "    return sentence\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"zoom delhi\"\n",
    "sentence2 = \"I am richa, i live in my fav city lucknow look for\"\n",
    "\n",
    "# Apply intent mapping\n",
    "processed_sentence1 = map_intents(sentence1, intent_map)  # \"zoom delhi\"\n",
    "processed_sentence2 = map_intents(sentence2, intent_map)  # \"zoom delhi\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91983dad-d9ac-4aff-ba44-bbf9808d2edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am richa, i live in my fav city lucknow zoom\n"
     ]
    }
   ],
   "source": [
    "print(processed_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75169b3d-68e0-40ef-9a58-ceaff4f5bf77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c039f5f-fc43-45f5-898c-2573f50f0caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'zoom', 'delhi', '[SEP]']\n",
      "['[CLS]', 'i', 'am', 'rich', '##a', ',', 'i', 'live', 'in', 'my', 'fa', '##v', 'city', 'lucknow', 'zoom', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    return tokens\n",
    "\n",
    "# Extract keywords after intent mapping\n",
    "keywords1 = extract_keywords(processed_sentence1)  # [\"zoom\", \"delhi\"]\n",
    "keywords2 = extract_keywords(processed_sentence2)  # [\"zoom\", \"delhi\"]\n",
    "\n",
    "print(keywords1)\n",
    "print(keywords2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3e1f9d-4311-4daa-b416-ef4890abdf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zoom', 'delhi']\n",
      "['zoom']\n"
     ]
    }
   ],
   "source": [
    "# Extract important keywords (manual filtering for this example)\n",
    "def filter_keywords(tokens):\n",
    "    # Assuming \"zoom\" and any proper nouns (like \"delhi\") are important\n",
    "    return [token for token in tokens if token in [\"zoom\", \"delhi\"]]\n",
    "\n",
    "# Filter tokens for keywords\n",
    "important_keywords1 = filter_keywords(keywords1)\n",
    "important_keywords2 = filter_keywords(keywords2)\n",
    "\n",
    "print(important_keywords1)  # [\"zoom\", \"delhi\"]\n",
    "print(important_keywords2)  # [\"zoom\", \"delhi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90de304e-86e2-42a7-856c-6f7d333ed14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 19:48:09.566823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 19:48:09.576783: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 19:48:09.580254: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 19:48:09.588611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-10 19:48:10.242597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: John, Type: PER, Confidence: 0.9968\n",
      "Entity: Google, Type: ORG, Confidence: 0.9990\n",
      "Entity: New York, Type: LOC, Confidence: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sds/geospatial-map/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained BERT-based NER pipeline\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"John works at Google in New York.\"\n",
    "\n",
    "# Perform NER\n",
    "ner_results = ner(sentence)\n",
    "\n",
    "# Print NER results\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Type: {entity['entity_group']}, Confidence: {entity['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd04eff-bdfe-4c3f-93c2-3876cf967260",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Palisade is beautiful\"\n",
    "\n",
    "# Perform NER\n",
    "ner_results = ner(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d20d1cd-b9c0-4a6b-bc2a-916d9d0bc4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.7049678,\n",
       "  'word': 'Palisade',\n",
       "  'start': 0,\n",
       "  'end': 8}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b49281-ec4b-49c0-9b1e-fe57f14b4fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load your fine-tuned command recognition model\n",
    "command_pipeline = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621fbee9-73df-409e-bb88-612f31fb6483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9989319443702698}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9952178001403809}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9814995527267456}]\n",
      "[{'label': 'POSITIVE', 'score': 0.999548614025116}]\n"
     ]
    }
   ],
   "source": [
    "# Example commands to classify\n",
    "commands = [\n",
    "    \"zoom in to Delhi\",\n",
    "    \"find directions to Bangalore\",\n",
    "    \"show me nearby restaurants\",\n",
    "    \"navigate to New York\"\n",
    "]\n",
    "\n",
    "# Perform command classification\n",
    "for command in commands:\n",
    "    result = command_pipeline(command)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e03b03-2440-4d00-9dfe-726062572426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in ./lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: numpy>=1.17 in ./lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: filelock in ./lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: requests in ./lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: xxhash in ./lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./lib/python3.10/site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in ./lib/python3.10/site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: pandas in ./lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./lib/python3.10/site-packages (from aiohttp->datasets) (1.14.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520c08a9-6d5c-4f5f-bef8-35f3ce6e0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa5b636-9b49-4a43-a65b-37adf3e73069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e023b4e9414eee9e987a9b8735679c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define command categories\n",
    "commands = [\n",
    "    {\"command\": \"zoom in to Delhi\", \"label\": \"zoom\"},\n",
    "    {\"command\": \"navigate to Bangalore\", \"label\": \"navigate\"},\n",
    "    {\"command\": \"find directions to New York\", \"label\": \"navigate\"},\n",
    "    {\"command\": \"show me nearby restaurants\", \"label\": \"find_nearby\"},\n",
    "    {\"command\": \"zoom out\", \"label\": \"zoom\"},\n",
    "    {\"command\": \"get me directions to Mumbai\", \"label\": \"navigate\"},\n",
    "    {\"command\": \"what's nearby?\", \"label\": \"find_nearby\"},\n",
    "    {\"command\": \"find a coffee shop in Delhi\", \"label\": \"find_nearby\"},\n",
    "]\n",
    "\n",
    "# Convert to Dataset\n",
    "dataset = Dataset.from_list(commands)\n",
    "\n",
    "# Encode labels\n",
    "label_dict = {label: i for i, label in enumerate(set(item[\"label\"] for item in commands))}\n",
    "dataset = dataset.map(lambda x: {\"labels\": label_dict[x[\"label\"]]})\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aec53e2-f1c4-4b3e-9b1a-bd1a2f9620ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'find_nearby': 0, 'navigate': 1, 'zoom': 2}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d9403a-63e1-4e71-bbbe-fb52f0c01a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ad3c77a3104aba9d2755eea59fa953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a771c662206b4729b1b783cad68462e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"command\"], truncation=True, padding='max_length', max_length=10)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format the dataset for PyTorch\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"command\", \"label\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"command\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c534d6b-63a1-43b4-b0ca-1a577d40b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sds/geospatial-map/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.459020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.506850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.518532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.0146769682566326, metrics={'train_runtime': 3.2705, 'train_samples_per_second': 5.504, 'train_steps_per_second': 0.917, 'total_flos': 92500810920.0, 'train_loss': 1.0146769682566326, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_dict))\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7120dcf-a0b8-45c4-b732-e61c95273a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('./fine_tuned_command_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4170ced9-a965-4d2a-a474-a09829828f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: zoom in to Delhi, Predicted Label: LABEL_2, Confidence: 0.4373\n",
      "Command: find directions to Bangalore, Predicted Label: LABEL_2, Confidence: 0.4058\n",
      "Command: show me nearby restaurants, Predicted Label: LABEL_2, Confidence: 0.4110\n",
      "Command: navigate to New York, Predicted Label: LABEL_2, Confidence: 0.4215\n",
      "Command: zoom out, Predicted Label: LABEL_2, Confidence: 0.4818\n"
     ]
    }
   ],
   "source": [
    "# Load the command recognition pipeline\n",
    "command_pipeline = pipeline(\"text-classification\", model='./fine_tuned_command_model', tokenizer=tokenizer)\n",
    "\n",
    "# Example commands to classify\n",
    "commands_to_classify = [\n",
    "    \"zoom in to Delhi\",\n",
    "    \"find directions to Bangalore\",\n",
    "    \"show me nearby restaurants\",\n",
    "    \"navigate to New York\",\n",
    "    \"zoom out\",\n",
    "]\n",
    "\n",
    "# Perform command classification\n",
    "for command in commands_to_classify:\n",
    "    result = command_pipeline(command)\n",
    "    print(f\"Command: {command}, Predicted Label: {result[0]['label']}, Confidence: {result[0]['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d96ee-ed42-40e1-8216-0c7c46dc3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'zoom',\n",
    "    1: 'navigate',\n",
    "    2: 'find_nearby'\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
